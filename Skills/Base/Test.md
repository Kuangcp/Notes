`目录 start`
 
- [测试](#测试)
    - [基础知识](#基础知识)
    - [测试类别](#测试类别)
        - [【A/B测试】](#ab测试)
            - [A/B测试的应用场景](#ab测试的应用场景)
                - [元素/控件层面](#元素控件层面)
                - [功能层面](#功能层面)
                - [产品层面](#产品层面)
                - [公司层面](#公司层面)
            - [相关问题](#相关问题)
        - [【冒烟测试】](#冒烟测试)

`目录 end` *目录创建于2018-01-21*
****************************************
# 测试

## 基础知识
> [码农翻身: 张大胖和单元测试](https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513524&idx=1&sn=8967771a4ac2a2b3b9bca019207e275a&chksm=80d67a77b7a1f36167ca66f9bdad810202d8790be1f15977dacd03348ee2b139f65ba58e4187&scene=21#wechat_redirect)

- [个人笔记: TDD 测试驱动开发](/Java/AdvancedLearning/ProgramThinking.md#tdd-测试驱动开发)


## 测试类别
### 【A/B测试】
> 对照实验，也叫随机实验和 A/B测试 [参考来源:知乎问题](https://www.zhihu.com/question/20045543) | [相关三方平台](http://www.appadhoc.com/)

>A/B测试其实是一种“先验”的实验体系，属于预测型结论，与“后验”的归纳性结论差别巨大。A/B测试的目的在于通过科学的实验设计、采样样本代表性、流量分割与小流量测试等方式来获得具有代表性的实验结论，并确信该结论在推广到全部流量可信。  
说句题外话，大量的大数据公司都在尝试通过“后验”结论进行未来行为预测，个人觉得然并卵，主要是因为数据不全、脏数据、随机事件、建模人为因素等等影响，方向无比正确，现实无比残酷

#### A/B测试的应用场景
- A/B测试这种方法论的应用非常广泛，这里讨论在Web产品、移动产品、数字广告优化领域的应用。应用场景由小到大可以可以分为：
    - 元素/控件层面
    - 功能层面
    - 产品层面
    - 公司层面

##### 元素/控件层面
灰度发布 和 A/B测试：重要页面的修改和流程上的调优，通过灰度发布到1%或者5%的用户，看其实际对用户的数据影响（访问时间增加、留存提高、下单率提高等），决定此修改到底是100%发布还是被砍掉
Google：
每个月从上百个A/B测试中找到十几个有效方案，月营收提升2%左右，10亿美元的规模
广告位左移一个像素带来X%的增收，左移两个像素带来Y%的亏损
任何产品改动需要A/B测试才能上线
Facebook：
6个月内所有版本完全线上灰度发布，通过不断进行用户流量分割的方式进行实验，获得无Bug口碑

> 灰度发布就是,先把一部分线上的业务分流到新系统上, 然后看情况, 再判断是否全部上线

##### 功能层面
无论是推荐算法还是定价策略
为了简单理解，说个定价策略，如上图。
一个价格包含这个几个因素：
>1.价格区间：  
用我最朴素的理解，人类是喜欢折扣的不理性动物：人们明显更乐意花45折买一个价值900块钱的东西而不是花67折买一个价值600块的东西，尽管东西一样，最终价格一样都是400块。  
所以你看电商广告都是打折配合几个垫背的低价来卖。。。

>2.价格精度：  
以前去超市经常能发现2.99元或者8.99，现在都变成2.32或者4.23，这是弄啥嘞？  
这里面太多心理学与营销的东西就不说了，在某些情况下，即使几分钱的价格变化对用户转化的影响是巨大的，比如一个东西原来卖400元，那现在改成399还是401可能对总营收的影响并不巨大，但是配合用户转化率的变化，可能营收的差异就天差地别了。

>3.价格周期：  
伴随着产品迭代、促销等等因素影响，什么时候降价是对自己最有利的策略，完全可以A/B测试来解决

##### 产品层面
A/B测试在产品层面的应用主要是通过“灰度发布”来实现的。
就目前移动端的产品来说，iOS的应用商店审核期是个大大大坑，任何BUG打补丁还得再来一遍，也就意味着补丁的审核期内用户带着BUG使用，这个太致命了，用户的获取成本高的吓人，因为这个流失太不值得了，基于A/B测试的灰度发布更重要的不是优化，而是保护性发布，先通过小流量的实际用户测试，有BUG或者新版本体验不好，可以立即回滚到老版本，简单有效。

##### 公司层面
A/B测试其实也是谷歌管理方法论，具体文章请参考：

- [Google 的产品策略：两边下注 ](https://mp.weixin.qq.com/s?__biz=MzAxMjYyNDQxMQ==&mid=207326881&idx=1&sn=1819eb391b8892ee44b2412df7e3e24b&scene=1&key=0acd51d81cb052bc0a3d80b5d616574c3980264cc57e79a82de989417043424df4559c1bdf2c5fcb4ed9236d4b71ec26&ascene=0&uin=NzEwNzE5MDIw&devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.10.4+build(14E46)&version=11020113&pass_ticket=N7rowGeMgSIo27M2UKuWi1JsLuQ2wgA2f6hPaARrza5teBqjdjGkx5S6hAcmdLxo)

#### 相关问题

>辛普森悖论（Simpson's Paradox）亦有人译为辛普森诡论，为英国统计学家E.H.辛普森（E.H.Simpson）于1951年提出的悖论，即在某个条件下的两组数据，分别讨论时都会满足某种性质，可是一旦合并考虑，却可能导致相反的结论。  
[相关:辛普森悖论：诡异的男女比例](https://www.guokr.com/article/6222/)

大多数的产品或功能上线前都会进行测试，实际上很多的测试行为并不科学，特别是很多定向的用户测试经常会有这个弊端，简单来说，如果新上线的一个功能，所有的研发工程师都说好，那是不是意味着所有的用户都觉得好？很多情况下是否定的。当然这个例子比较简单，实际上很多A/B测试方法并没有考虑到这个问题，以至于最后得出的结论跟实际情况差异巨大。
要解决这个问题，对采样、聚类、流量分割等要求非常的高，这也是为什么A/B测试工具不能像很多统计工具一样，埋个点看数据，再根据数据反推业务逻辑，而是要充分与业务结合，从一开始就应该考虑业务策略，让用户去选择适合其口味的产品。
 现在的创业者面临着前所未有的竞争压力，好的想法与用户接受的想法有着各种不可逾越的鸿沟。特别是伴随着激烈的竞争，谁能领先一步可能就变成了赢者通吃的局面。

> 相关专栏: [[A/B]那些年，我们追过的AB Testing （一）](https://zhuanlan.zhihu.com/milmax/20394728) | [[A/B]那些年，我们追过的AB Testing （二）从“People you may know”到growth hacking](https://zhuanlan.zhihu.com/milmax/20507473)

### 【冒烟测试】
> 冒烟测试源自硬件行业，对一个硬件或者硬件组件改动后，直接给设备加电，看看设备会不会冒烟，没冒烟，就表示待测组件是通过了测试。  
> 目的是确认软件基本功能正常，可以进行后续的正式测试工作。